{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from geoimages import etl\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip files, rename to include lables, generate metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 29998\n"
     ]
    }
   ],
   "source": [
    "## unzip images, create and save json with image metadata\n",
    "images_md = etl.Images()\n",
    "images_md.unzip_images('../datasets/geotechnical_images.zip')\n",
    "images_md.rename_images()\n",
    "md_all = images_md.generate_image_metadata()\n",
    "images_md.write_json(md_all,'../datasets/metadata_all.json')\n",
    "print('Total images: ' + str(len(md_all['images'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training set images: 20998\n"
     ]
    }
   ],
   "source": [
    "## randomly sample images for training, save json with image metadata\n",
    "images_train = etl.Images()\n",
    "images_train.dev_path = '../images/train'\n",
    "images_train.sample_images(sample_percent=0.70)\n",
    "images_train.images_path = '../images/train'\n",
    "md_train = images_train.generate_image_metadata()\n",
    "images_train.write_json(md_train,'../datasets/metadata_train.json')\n",
    "print('Total training set images: ' + str(len(images_train.images_meta_data['images'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train shape: (20998, 28, 28, 3)\n",
      "Y Train shape: (20998,)\n"
     ]
    }
   ],
   "source": [
    "## etl training data, save\n",
    "X_Train_Orig, Y_Train_Orig = images_train.images_to_x_y()\n",
    "hf = h5py.File('../datasets/image_classification_train.h5', 'w')\n",
    "hf.create_dataset('X_Train_Orig', data=X_Train_Orig, compression=\"gzip\", compression_opts=9)\n",
    "hf.create_dataset('Y_Train_Orig', data=Y_Train_Orig.astype('S'), compression=\"gzip\", compression_opts=9)\n",
    "hf.close()\n",
    "print('X_Train_Orig shape: ' + str(X_Train_Orig.shape))\n",
    "print('Y_Train_Orig shape: ' + str(Y_Train_Orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total development set images: 5400\n"
     ]
    }
   ],
   "source": [
    "## randomly sample images for dev, save json with image metadata\n",
    "images_dev = etl.Images()\n",
    "images_dev.dev_path = '../images/dev'\n",
    "images_dev.sample_images(sample_percent=0.60)\n",
    "images_dev.images_path = '../images/dev'\n",
    "md_dev = images_dev.generate_image_metadata()\n",
    "images_dev.write_json(md_dev,'../datasets/metadata_dev.json')\n",
    "print('Total development set images: ' + str(len(images_dev.images_meta_data['images'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Dev_Orig shape: (5400, 28, 28, 3)\n",
      "Y_Dev_Orig shape: (5400,)\n"
     ]
    }
   ],
   "source": [
    "## etl development data, save\n",
    "X_Dev_Orig, Y_Dev_Orig = images_dev.images_to_x_y()\n",
    "hf = h5py.File('../datasets/image_classification_dev.h5', 'w')\n",
    "hf.create_dataset('X_Dev_Orig', data=X_Dev_Orig, compression=\"gzip\", compression_opts=9)\n",
    "hf.create_dataset('Y_Dev_Orig', data=Y_Dev_Orig.astype('S'), compression=\"gzip\", compression_opts=9)\n",
    "hf.close()\n",
    "print('X_Dev_Orig shape: ' + str(X_Dev_Orig.shape))\n",
    "print('Y_Dev_Orig shape: ' + str(Y_Dev_Orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test set images: 3564\n"
     ]
    }
   ],
   "source": [
    "## randomly sample images for test, save json with image metadata\n",
    "images_test = etl.Images()\n",
    "images_test.dev_path = '../images/test'\n",
    "images_test.sample_images(sample_percent=0.99)\n",
    "images_test.images_path = '../images/test'\n",
    "md_test = images_test.generate_image_metadata()\n",
    "images_test.write_json(md_test,'../datasets/metadata_test.json')\n",
    "print('Total test set images: ' + str(len(images_test.images_meta_data['images'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Test_Orig shape: (3564, 28, 28, 3)\n",
      "Y_Test_Orig shape: (3564,)\n"
     ]
    }
   ],
   "source": [
    "## etl test data, save\n",
    "X_Test_Orig, Y_Test_Orig = images_test.images_to_x_y()\n",
    "hf = h5py.File('../datasets/image_classification_test.h5', 'w')\n",
    "hf.create_dataset('X_Test_Orig', data=X_Test_Orig, compression=\"gzip\", compression_opts=9)\n",
    "hf.create_dataset('Y_Test_Orig', data=Y_Test_Orig.astype('S'), compression=\"gzip\", compression_opts=9)\n",
    "hf.close()\n",
    "print('X_Test_Orig shape: ' + str(X_Test_Orig.shape))\n",
    "print('Y_Test_Orig shape: ' + str(Y_Test_Orig.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
